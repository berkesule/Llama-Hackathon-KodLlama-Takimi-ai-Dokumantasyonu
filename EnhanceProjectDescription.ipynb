{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "FGYZFn2ye2YY",
        "outputId": "69e63143-787f-4395-b0b2-d3343c2001f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vllm==0.10.2\n",
            "  Downloading vllm-0.10.2-cp38-abi3-manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2024.11.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.67.1)\n",
            "Collecting blake3 (from vllm==0.10.2)\n",
            "  Downloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.55.2 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.57.1)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.22.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (5.29.5)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2) (0.119.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (3.13.1)\n",
            "Requirement already satisfied: openai>=1.99.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (1.109.1)\n",
            "Requirement already satisfied: pydantic>=2.11.7 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.11.10)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.23.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (11.3.0)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.10.2)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.12.0)\n",
            "Collecting lm-format-enforcer==0.11.3 (from vllm==0.10.2)\n",
            "  Downloading lm_format_enforcer-0.11.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llguidance<0.8.0,>=0.7.11 (from vllm==0.10.2)\n",
            "  Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting outlines_core==0.2.11 (from vllm==0.10.2)\n",
            "  Downloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting diskcache==5.6.3 (from vllm==0.10.2)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting lark==1.2.2 (from vllm==0.10.2)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar==0.1.23 (from vllm==0.10.2)\n",
            "  Downloading xgrammar-0.1.23-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.15.0)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (3.20.0)\n",
            "Collecting partial-json-parser (from vllm==0.10.2)\n",
            "  Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (26.2.1)\n",
            "Collecting msgspec (from vllm==0.10.2)\n",
            "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf>=0.13.0 (from vllm==0.10.2)\n",
            "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting mistral_common>=1.8.2 (from mistral_common[audio,image]>=1.8.2->vllm==0.10.2)\n",
            "  Downloading mistral_common-1.8.5-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.12.0.88)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (6.0.3)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (1.17.0)\n",
            "Collecting setuptools<80,>=77.0.3 (from vllm==0.10.2)\n",
            "  Downloading setuptools-79.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.8.1)\n",
            "Collecting compressed-tensors==0.11.0 (from vllm==0.10.2)\n",
            "  Downloading compressed_tensors-0.11.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting depyf==0.19.0 (from vllm==0.10.2)\n",
            "  Downloading depyf-0.19.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (3.1.1)\n",
            "Collecting watchfiles (from vllm==0.10.2)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (1.16.2)\n",
            "Collecting ninja (from vllm==0.10.2)\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pybase64 (from vllm==0.10.2)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Collecting cbor2 (from vllm==0.10.2)\n",
            "  Downloading cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting setproctitle (from vllm==0.10.2)\n",
            "  Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
            "Collecting openai-harmony>=0.0.3 (from vllm==0.10.2)\n",
            "  Downloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Collecting numba==0.61.2 (from vllm==0.10.2)\n",
            "  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting ray>=2.48.0 (from ray[cgraph]>=2.48.0->vllm==0.10.2)\n",
            "  Downloading ray-2.50.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio==2.8.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision==0.23.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.23.0+cu126)\n",
            "Collecting xformers==0.0.32.post1 (from vllm==0.10.2)\n",
            "  Downloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.11.0->vllm==0.10.2) (2.4.6)\n",
            "Collecting astor (from depyf==0.19.0->vllm==0.10.2)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from depyf==0.19.0->vllm==0.10.2) (0.3.8)\n",
            "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.11.3->vllm==0.10.2)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lm-format-enforcer==0.11.3->vllm==0.10.2) (25.0)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm==0.10.2)\n",
            "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (3.4.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.48.0)\n",
            "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading fastapi_cli-0.0.14-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2) (0.0.20)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.38.0)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (4.25.1)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2)\n",
            "  Downloading pydantic_extra_types-2.10.6-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm==0.10.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm==0.10.2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm==0.10.2) (0.4.2)\n",
            "Collecting click!=8.3.0,>=7.0 (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm==0.10.2)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm==0.10.2) (1.1.2)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm==0.10.2) (13.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (2025.10.5)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.21.1->vllm==0.10.2) (0.35.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.2->vllm==0.10.2) (0.6.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (1.22.0)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.20.0)\n",
            "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading rich_toolkit-0.15.1-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading fastapi_cloud_cli-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.21.1->vllm==0.10.2) (1.1.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->vllm==0.10.2) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (0.27.1)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->vllm==0.10.2) (1.3.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm==0.10.2) (0.8.3)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (0.13.1)\n",
            "Requirement already satisfied: soxr>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (1.0.0)\n",
            "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading rignore-0.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (2.42.1)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (13.9.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (2.0.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (2.23)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.1.2)\n",
            "Downloading vllm-0.10.2-cp38-abi3-manylinux1_x86_64.whl (436.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.11.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading depyf-0.19.0-py3-none-any.whl (39 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.11.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m124.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgrammar-0.1.23-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m126.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.8.5-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m138.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading ray-2.50.1-cp312-cp312-manylinux2014_x86_64.whl (71.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-79.0.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (388 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (285 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.7/285.7 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl (10 kB)\n",
            "Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (32 kB)\n",
            "Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
            "Downloading fastapi_cli-0.0.14-py3-none-any.whl (11 kB)\n",
            "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.6-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi_cloud_cli-0.3.1-py3-none-any.whl (19 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_toolkit-0.15.1-py3-none-any.whl (29 kB)\n",
            "Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rignore-0.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (951 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m951.9/951.9 kB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvloop, setuptools, setproctitle, rignore, pycountry, pybase64, partial-json-parser, outlines_core, ninja, msgspec, llvmlite, llguidance, lark, interegular, httptools, gguf, dnspython, diskcache, click, cbor2, blake3, astor, watchfiles, numba, email-validator, depyf, rich-toolkit, pydantic-extra-types, prometheus-fastapi-instrumentator, openai-harmony, lm-format-enforcer, xformers, ray, fastapi-cloud-cli, fastapi-cli, xgrammar, mistral_common, compressed-tensors, vllm\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: lark\n",
            "    Found existing installation: lark 1.3.0\n",
            "    Uninstalling lark-1.3.0:\n",
            "      Successfully uninstalled lark-1.3.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.0\n",
            "    Uninstalling click-8.3.0:\n",
            "      Successfully uninstalled click-8.3.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed astor-0.8.1 blake3-1.0.8 cbor2-5.7.1 click-8.2.1 compressed-tensors-0.11.0 depyf-0.19.0 diskcache-5.6.3 dnspython-2.8.0 email-validator-2.3.0 fastapi-cli-0.0.14 fastapi-cloud-cli-0.3.1 gguf-0.17.1 httptools-0.7.1 interegular-0.3.3 lark-1.2.2 llguidance-0.7.30 llvmlite-0.44.0 lm-format-enforcer-0.11.3 mistral_common-1.8.5 msgspec-0.19.0 ninja-1.13.0 numba-0.61.2 openai-harmony-0.0.4 outlines_core-0.2.11 partial-json-parser-0.2.1.1.post6 prometheus-fastapi-instrumentator-7.1.0 pybase64-1.4.2 pycountry-24.6.1 pydantic-extra-types-2.10.6 ray-2.50.1 rich-toolkit-0.15.1 rignore-0.7.1 setproctitle-1.3.7 setuptools-79.0.1 uvloop-0.22.1 vllm-0.10.2 watchfiles-1.1.1 xformers-0.0.32.post1 xgrammar-0.1.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "f0e7d8ed699b4810899de9778a737c80"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install vllm==0.10.2 #Kullandığımız LLaMA versiyonu için downgrade gerekli\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub[pandas-datasets]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gwkisVcohuLE",
        "outputId": "9821ff90-c907-4d36-a093-979ff896444a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub[pandas-datasets] in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "kagglehub.dataset_download('subhranilmondal12/project-planning-data', path='/content/data.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "qbfoSJ39nM7Y",
        "outputId": "decdea31-52fe-437c-ba3c-4ce5ead3e086"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'project-planning-data' dataset.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "'/content/data.csv' is not present in the dataset files. You can access the other files of the attached dataset at '/kaggle/input/project-planning-data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1304860727.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'subhranilmondal12/project-planning-data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mdataset_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_dataset_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Downloading Dataset: {h.to_url()} ...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mEXTRA_CONSOLE_BLOCK\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle, path, force_download)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mSome\u001b[0m \u001b[0mcases\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mmight\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCompetition\u001b[0m \u001b[0mdatasource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbased\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/colab_cache_resolver.py\u001b[0m in \u001b[0;36m_resolve\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0;34mf\"You can access the other files of the attached dataset at '{cached_path}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 )\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcached_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: '/content/data.csv' is not present in the dataset files. You can access the other files of the attached dataset at '/kaggle/input/project-planning-data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!vllm serve ytu-ce-cosmos/Turkish-Llama-8b-DPO-v0.1 --trust-remote-code --port 8000 --dtype bfloat16 --max-model-len 8192 --gpu-memory-utilization 0.90 --disable-custom-all-reduce --enforce-eager\n",
        "#pip install pyngrok\n",
        "#ngrok config add-authtoken $YOUR_AUTHTOKEN //export YOUR_AUTHTOKEN \"token\"\n",
        "#ngrok http --url=leopard-loyal-monkfish.ngrok-free.app 8000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zOfjrK9gIfu",
        "outputId": "69be1cc0-2509-4853-fc24-1b4e3ac6bffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-26 07:05:46.283876: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-10-26 07:05:46.300660: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761462346.321624    2759 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761462346.327995    2759 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761462346.344330    2759 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761462346.344355    2759 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761462346.344358    2759 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761462346.344361    2759 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-26 07:05:46.349168: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 10-26 07:05:53 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:05:58 [api_server.py:1896] vLLM API server version 0.10.2\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:05:58 [utils.py:328] non-default args: {'model_tag': 'ytu-ce-cosmos/Turkish-Llama-8b-DPO-v0.1', 'model': 'ytu-ce-cosmos/Turkish-Llama-8b-DPO-v0.1', 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 8192, 'enforce_eager': True, 'disable_custom_all_reduce': True}\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "config.json: 100% 727/727 [00:00<00:00, 6.66MB/s]\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:06:15 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:06:15 [__init__.py:1815] Using max model len 8192\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:06:17 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:06:17 [__init__.py:3400] Cudagraph is disabled under eager mode\n",
            "tokenizer_config.json: 51.0kB [00:00, 106MB/s]\n",
            "tokenizer.json: 9.09MB [00:00, 224MB/s]\n",
            "special_tokens_map.json: 100% 296/296 [00:00<00:00, 3.07MB/s]\n",
            "generation_config.json: 100% 172/172 [00:00<00:00, 1.95MB/s]\n",
            "2025-10-26 07:06:25.053489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761462385.074499    3039 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761462385.080808    3039 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761462385.096367    3039 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761462385.096410    3039 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761462385.096413    3039 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761462385.096417    3039 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 10-26 07:06:30 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m INFO 10-26 07:06:32 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m INFO 10-26 07:06:32 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ytu-ce-cosmos/Turkish-Llama-8b-DPO-v0.1', speculative_config=None, tokenizer='ytu-ce-cosmos/Turkish-Llama-8b-DPO-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ytu-ce-cosmos/Turkish-Llama-8b-DPO-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
            "[W1026 07:06:34.360511888 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m INFO 10-26 07:06:34 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m WARNING 10-26 07:06:34 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m INFO 10-26 07:06:34 [gpu_model_runner.py:2338] Starting to load model ytu-ce-cosmos/Turkish-Llama-8b-DPO-v0.1...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m INFO 10-26 07:06:34 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m INFO 10-26 07:06:34 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m INFO 10-26 07:06:35 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
            "model-00001-of-00004.safetensors: 100% 4.98G/4.98G [00:24<00:00, 202MB/s]\n",
            "model-00002-of-00004.safetensors: 100% 5.00G/5.00G [00:26<00:00, 187MB/s]\n",
            "model-00003-of-00004.safetensors: 100% 4.92G/4.92G [00:24<00:00, 197MB/s]\n",
            "model-00004-of-00004.safetensors: 100% 1.17G/1.17G [00:05<00:00, 195MB/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m INFO 10-26 07:08:00 [weight_utils.py:369] Time spent downloading weights for ytu-ce-cosmos/Turkish-Llama-8b-DPO-v0.1: 84.589095 seconds\n",
            "model.safetensors.index.json: 23.9kB [00:00, 95.9MB/s]\n",
            "Loading safetensors checkpoint shards: 100% 4/4 [00:04<00:00,  1.10s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m INFO 10-26 07:08:04 [default_loader.py:268] Loading weights took 4.51 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m INFO 10-26 07:08:05 [gpu_model_runner.py:2392] Model loading took 14.9596 GiB and 90.133098 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m INFO 10-26 07:08:07 [gpu_worker.py:298] Available KV cache memory: 19.43 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m INFO 10-26 07:08:07 [kv_cache_utils.py:864] GPU KV cache size: 159,184 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m INFO 10-26 07:08:07 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 19.43x\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m INFO 10-26 07:08:07 [gpu_worker.py:391] Free memory on device (39.07/39.56 GiB) on startup. Desired GPU memory utilization is (0.9, 35.6 GiB). Actual usage is 14.96 GiB for weight, 1.19 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.0 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=20709119385` to fit into requested memory, or `--kv-cache-memory=24432344064` to fully utilize gpu memory. Current kv cache memory in use is 20866405785 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m INFO 10-26 07:08:07 [core.py:218] init engine (profile, create kv cache, warmup model) took 2.30 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m INFO 10-26 07:08:09 [__init__.py:3400] Cudagraph is disabled under eager mode\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:09 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 9949\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:09 [async_llm.py:180] Torch profiler disabled. AsyncLLM CPU traces will not be collected.\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:09 [api_server.py:1692] Supported_tasks: ['generate']\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m WARNING 10-26 07:08:09 [__init__.py:1695] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:09 [serving_responses.py:130] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [serving_chat.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [api_server.py:1971] Starting vLLM API server 0 on http://0.0.0.0:8000\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:36] Available routes are:\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /openapi.json, Methods: HEAD, GET\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /docs, Methods: HEAD, GET\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /redoc, Methods: HEAD, GET\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /health, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /load, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /ping, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /ping, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /tokenize, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /detokenize, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /v1/models, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /version, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /v1/responses, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /v1/chat/completions, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /v1/completions, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /v1/embeddings, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /pooling, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /classify, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /score, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /v1/score, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /v1/audio/translations, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /rerank, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /v1/rerank, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /v2/rerank, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /invocations, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:08:10 [launcher.py:44] Route: /metrics, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m2759\u001b[0m]\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m \u001b[32mINFO\u001b[0m:     193.140.4.13:0 - \"\u001b[1mOPTIONS /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:22:32 [chat_utils.py:538] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3039)\u001b[0;0m WARNING 10-26 07:22:32 [cudagraph_dispatcher.py:102] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:22:42 [loggers.py:123] Engine 000: Avg prompt throughput: 21.1 tokens/s, Avg generation throughput: 51.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:22:52 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 52.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m \u001b[32mINFO\u001b[0m:     193.140.4.13:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:23:02 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:23:12 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m \u001b[32mINFO\u001b[0m:     193.140.4.13:0 - \"\u001b[1mOPTIONS /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:29:22 [loggers.py:123] Engine 000: Avg prompt throughput: 16.5 tokens/s, Avg generation throughput: 33.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 21.3%\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:29:32 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 53.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 21.3%\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m \u001b[32mINFO\u001b[0m:     193.140.4.13:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:29:42 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 21.3%\n",
            "\u001b[1;36m(APIServer pid=2759)\u001b[0;0m INFO 10-26 07:29:52 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 21.3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from textwrap import dedent\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "DATA_PATH = '/content/project_plan_final_data.csv'\n",
        "OUTPUT_PATH = '/content/project_plan_final_data_turkish.csv'\n",
        "MODEL = 'ytu-ce-cosmos/Turkish-Llama-8b-DPO-v0.1'\n",
        "NGROK_URL = 'https://leopard-loyal-monkfish.ngrok-free.app'\n",
        "TEMPERATURE = 0.3\n",
        "\n",
        "TRANSLATION_PROMPT = \"\"\"Sen profesyonel bir çevirmensin. Aşağıdaki metni Türkçe'ye çevir.\n",
        "Teknik terimleri ve profesyonel ifadeleri koruyarak, anlaşılır ve akıcı bir çeviri yap.\n",
        "Sadece çeviriyi ver, başka yorum ekleme.\n",
        "\n",
        "ÇEVİRİLECEK METİN:\n",
        "{text}\n",
        "\n",
        "TÜRKÇE ÇEVİRİ:\"\"\"\n",
        "\n",
        "\n",
        "def setup_client():\n",
        "    base = NGROK_URL.rstrip('/')\n",
        "    if not base.endswith('/v1'):\n",
        "        base += '/v1'\n",
        "    return OpenAI(base_url=base, api_key=\"sk-local-not-needed\")\n",
        "\n",
        "\n",
        "def translate_text(client, text, model=MODEL):\n",
        "    \"\"\"Tek bir metni Türkçe'ye çevir\"\"\"\n",
        "    if pd.isna(text) or not text.strip():\n",
        "        return text\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": TRANSLATION_PROMPT.format(text=text)}\n",
        "            ],\n",
        "            temperature=TEMPERATURE,\n",
        "            max_tokens=4000\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Çeviri hatası: {e}\")\n",
        "        return text\n",
        "\n",
        "\n",
        "def translate_csv():\n",
        "    \"\"\"CSV'deki tüm verileri Türkçe'ye çevir\"\"\"\n",
        "    print(\"📂 CSV dosyası okunuyor...\")\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    print(f\"✅ {len(df)} satır yüklendi\\n\")\n",
        "\n",
        "    client = setup_client()\n",
        "\n",
        "    df['Project_description_TR'] = ''\n",
        "    df['Project_plan_TR'] = ''\n",
        "\n",
        "    print(\"🔄 Çeviri başlıyor...\\n\")\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Çeviriliyor\"):\n",
        "        if 'Project_description' in df.columns:\n",
        "            print(f\"📝 Satır {idx+1}/{len(df)} - Description çeviriliyor...\")\n",
        "            df.at[idx, 'Project_description_TR'] = translate_text(\n",
        "                client,\n",
        "                row['Project_description']\n",
        "            )\n",
        "\n",
        "        if 'Project_plan' in df.columns:\n",
        "            print(f\"📋 Satır {idx+1}/{len(df)} - Plan çeviriliyor...\")\n",
        "            df.at[idx, 'Project_plan_TR'] = translate_text(\n",
        "                client,\n",
        "                row['Project_plan']\n",
        "            )\n",
        "\n",
        "        print(f\"✅ Satır {idx+1} tamamlandı\\n\")\n",
        "\n",
        "    print(f\"💾 Çevrilen veriler kaydediliyor: {OUTPUT_PATH}\")\n",
        "    df.to_csv(OUTPUT_PATH, index=False, encoding='utf-8')\n",
        "    print(f\"✅ Tamamlandı! {len(df)} satır çevrildi ve kaydedildi.\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    translated_df = translate_csv()\n",
        "\n",
        "    print(\"\\n📊 İlk birkaç çeviri örneği:\")\n",
        "    print(\"=\"*80)\n",
        "    for idx, row in translated_df.head(2).iterrows():\n",
        "        print(f\"\\n--- SATIR {idx+1} ---\")\n",
        "        if 'Project_description' in translated_df.columns:\n",
        "            print(f\"\\nORİJİNAL: {row['Project_description'][:200]}...\")\n",
        "            print(f\"\\nTÜRKÇE: {row['Project_description_TR'][:200]}...\")\n",
        "        print(\"=\"*80)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "u9xoTFxxzlAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "DATA_PATH = '/content/project_plan_final_data_turkish.csv'\n",
        "OUTPUT_PATH = '/content/project_plan_final_data_turkish_sum.csv'\n",
        "MODEL = 'ytu-ce-cosmos/Turkish-Llama-8b-DPO-v0.1'\n",
        "NGROK_URL = 'https://leopard-loyal-monkfish.ngrok-free.app'\n",
        "TEMPERATURE = 0.3\n",
        "\n",
        "EXPLAIN_PROMPT = \"\"\"Sen profesyonel bir problem anlatıcısısın. Aşağıdaki problemi detaylı bir şekilde anlat.\n",
        "Teknik terimleri ve profesyonel ifadeleri koruyarak, anlaşılır ve akıcı bir açıklama yap.\n",
        "Sadece açıklamayı ver, başka yorum ekleme.\n",
        "\n",
        "AÇIKLANACAK METİN:\n",
        "{text}\n",
        "\n",
        "YAPILAN AÇIKLAMA:\"\"\"\n",
        "\n",
        "def setup_client():\n",
        "    base = NGROK_URL.rstrip('/')\n",
        "    if not base.endswith('/v1'):\n",
        "        base += '/v1'\n",
        "    return OpenAI(base_url=base, api_key=\"sk-local-not-needed\")\n",
        "\n",
        "def _format_prompt_text(raw_text: str) -> str:\n",
        "    return raw_text.replace('{', '{{').replace('}', '}}')\n",
        "\n",
        "def explain_text(client, text, model=MODEL):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text).strip()\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": EXPLAIN_PROMPT.format(text=_format_prompt_text(text))}],\n",
        "            temperature=TEMPERATURE,\n",
        "            max_tokens=4000\n",
        "        )\n",
        "        return (response.choices[0].message.content or \"\").strip()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Açıklama hatası: {e}\")\n",
        "        return text\n",
        "\n",
        "def explain_csv():\n",
        "    print(\"📂 CSV dosyası okunuyor...\")\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    print(f\"✅ {len(df)} satır yüklendi\\n\")\n",
        "\n",
        "    if 'Project_plan_TR' not in df.columns:\n",
        "        raise KeyError(\"Gerekli kolon bulunamadı: 'Project_plan_TR'\")\n",
        "\n",
        "    client = setup_client()\n",
        "    if 'Project_explanation_TR' not in df.columns:\n",
        "        df['Project_explanation_TR'] = ''\n",
        "\n",
        "    print(\"🔄 Açıklama üretimi başlıyor...\\n\")\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Açıklanıyor\"):\n",
        "        df.at[idx, 'Project_explanation_TR'] = explain_text(\n",
        "            client,\n",
        "            row['Project_plan_TR']\n",
        "        )\n",
        "\n",
        "    print(f\"💾 Açıklanan veriler kaydediliyor: {OUTPUT_PATH}\")\n",
        "    df.to_csv(OUTPUT_PATH, index=False, encoding='utf-8')\n",
        "    print(f\"✅ Tamamlandı! {len(df)} satır açıklandı ve kaydedildi.\\n\")\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    explained_df = explain_csv()\n",
        "\n",
        "    print(\"\\n📊 İlk birkaç Açıklama örneği:\")\n",
        "    print(\"=\"*80)\n",
        "    for idx, row in explained_df.head(2).iterrows():\n",
        "        print(f\"\\n--- SATIR {idx+1} ---\")\n",
        "        print(f\"\\nAÇIKLAMA (TR): {str(row['Project_explanation_TR'])[:200]}...\")\n",
        "        print(\"=\"*80)\n"
      ],
      "metadata": {
        "id": "XN0IWsG0ErUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from textwrap import dedent\n",
        "import pandas as pd\n",
        "\n",
        "DATA_PATH = '/content/project_explanation_tr_only.csv'\n",
        "MODEL = 'ytu-ce-cosmos/Turkish-Llama-8b-DPO-v0.1'\n",
        "NGROK_URL = 'https://leopard-loyal-monkfish.ngrok-free.app'  # OpenAI-compatible base\n",
        "NUMBER_OF_SHOTS = 3\n",
        "TEMPERATURE = 0.7\n",
        "SYS_PROMPT = \"\"\"Sen profesyonel bir problem anlatıcısısın. Aşağıdaki KÖTÜ_AÇIKLAMA’yı zenginleştirerek problemi detaylı bir şekilde anlat.\n",
        "Teknik terimleri ve profesyonel ifadeleri koruyarak, anlaşılır ve akıcı bir açıklama yap.\n",
        "Sadece açıklamayı ver, başka yorum ekleme.\n",
        "\n",
        "KÖTÜ AÇIKLAMA:\n",
        "{text}\n",
        "\n",
        "YAPILAN AÇIKLAMA:\\n\\nEXAMPLES:\\n\\n\"\"\"\n",
        "\n",
        "\n",
        "USR_PROMPT = \"\"\"I need you to generate a detailed problem explaining behind professional project plan. Analyze the examples below to understand the expected level of detail.\n",
        "Give only one explanation.\n",
        "\n",
        "{examples}\n",
        "\n",
        "─────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "Now create a comprehensive problem defining for:\n",
        "\n",
        "KÖTÜ AÇIKLAMA:\n",
        "{user_input}\n",
        "\n",
        "YAPILAN AÇIKLAMA:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ProjectPlanner:\n",
        "    def __init__(self, csv_path, api_key=None, model=MODEL, base_url=None):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.model = model\n",
        "\n",
        "        if base_url:\n",
        "            # vLLM/OpenAI compatible sunucular için /v1 şart\n",
        "            base = base_url.rstrip('/')\n",
        "            if not base.endswith('/v1'):\n",
        "                base += '/v1'\n",
        "            # Lokal/özel sunucu çoğunlukla API key doğrulamaz; SDK bir değer istediği için dummy veriyoruz\n",
        "            self.client = OpenAI(base_url=base, api_key=api_key or \"sk-local-not-needed\")\n",
        "        else:\n",
        "            # Gerçek OpenAI kullanacaksan burada env'den anahtar okuyabilir\n",
        "            self.client = OpenAI(api_key=api_key)\n",
        "\n",
        "    # 🔁 Yalnızca burayı NUMBER_OF_SHOTS adet rastgele seçim yapacak şekilde düzenledik\n",
        "    def get_examples(self, n=None, seed=None):\n",
        "        \"\"\"CSV içinden rastgele n satır seç (n verilmezse NUMBER_OF_SHOTS).\"\"\"\n",
        "        k = int(min(n if n is not None else NUMBER_OF_SHOTS, len(self.df)))\n",
        "        return self.df.sample(n=k, random_state=seed)\n",
        "\n",
        "    def generate(self, user_input, n_examples=NUMBER_OF_SHOTS, temperature=TEMPERATURE):\n",
        "        examples = self.get_examples(n_examples)\n",
        "\n",
        "        prompt = dedent(USR_PROMPT).strip()\n",
        "        # Numara sıralı olsun diye reset_index\n",
        "        for ex_idx, row in examples.reset_index(drop=True).iterrows():\n",
        "            prompt += f\"### Example {ex_idx + 1}\\n\"\n",
        "            prompt += f\"Input: {row['Project_explanation_TR'].split('.')[0]}\\n\\n\"\n",
        "            prompt += f\"Output: {row['Project_explanation_TR']}\\n\\n---\\n\\n\"\n",
        "\n",
        "        prompt += f\"### Now generate for:\\nInput: {user_input}\\n\\nOutput:\"\n",
        "\n",
        "        print(f\"🔄 Generating with {self.model} (using {n_examples} examples)...\")\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": dedent(SYS_PROMPT).strip()},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=4000\n",
        "        )\n",
        "\n",
        "        plan = response.choices[0].message.content\n",
        "\n",
        "        # usage alanı sunucuya göre değişebilir: prompt/completion vs input/output\n",
        "        usage = getattr(response, \"usage\", None)\n",
        "        if usage:\n",
        "            # SDK objesi veya dict olabilir; her iki adı da deneriz\n",
        "            total = getattr(usage, \"total_tokens\", None) or getattr(usage, \"total\", None) or getattr(usage, \"totalTokens\", None) or (hasattr(usage, \"get\") and usage.get(\"total_tokens\"))\n",
        "            in_tok = (\n",
        "                getattr(usage, \"prompt_tokens\", None)\n",
        "                or getattr(usage, \"input_tokens\", None)\n",
        "                or (hasattr(usage, \"get\") and (usage.get(\"prompt_tokens\") or usage.get(\"input_tokens\")))\n",
        "            )\n",
        "            out_tok = (\n",
        "                getattr(usage, \"completion_tokens\", None)\n",
        "                or getattr(usage, \"output_tokens\", None)\n",
        "                or (hasattr(usage, \"get\") and (usage.get(\"completion_tokens\") or usage.get(\"output_tokens\")))\n",
        "            )\n",
        "            print(f\"💰 Tokens: {total} (in: {in_tok}, out: {out_tok})\")\n",
        "\n",
        "        print(f\"✅ Generated! Length: {len(plan)} chars\\n\")\n",
        "        return plan\n",
        "\n",
        "\n",
        "def use_with_ngrok(user_input):\n",
        "    planner = ProjectPlanner(\n",
        "        csv_path=DATA_PATH,\n",
        "        api_key=\"sk-local-not-needed\",   # çoğu OpenAI-compatible sunucu için fark etmez\n",
        "        base_url=NGROK_URL,              # __init__ artık kabul ediyor\n",
        "        model=MODEL\n",
        "    )\n",
        "    plan = planner.generate(user_input)\n",
        "    print(plan)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    user_input = \"PlanLLaMA adında bir ürün yönetim sistemi oluşturmak istiyor\"\n",
        "    use_with_ngrok()\n"
      ],
      "metadata": {
        "id": "2YZrrY1ttY3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "from typing import Tuple, List\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Sistem promptu\n",
        "# -----------------------------\n",
        "system_prompt = \"\"\"AŞAĞIDAKİ TALİMATLARI DİKKATLE İZLE.\n",
        "\n",
        "ROLÜN: Kısa özgeçmiş (CV) metinlerinden ilgi alanları ve yetkinlikleri çıkaran uzman bir İK analisti ve NLP etiketleyicisisin.\n",
        "\n",
        "GÖREV: Verilen düz metin CV içinden ({{cv_text}}) adayın\n",
        "1) İlgilendiği alanları (konu/tema/domain),\n",
        "2) Yetkinliklerini (programlama dilleri, kütüphaneler, araçlar, alan bilgisi, yumuşak beceriler, konuşulan diller)\n",
        "çıkar ve AŞAĞIDAKİ KATI JSON ŞEMASI ile döndür.\n",
        "\n",
        "ÇIKTI SADECE JSON OLSUN. Gerekçe, açıklama, adım adım düşünme, ön yazı YOK.\n",
        "\n",
        "### TANIMLAR\n",
        "- İlgi alanı (interest): Adayın çalışmayı sevdiği/odaklandığı konu başlığı (örn: “Yapay Zeka/LLM”, “Belge İşleme/OCR”, “Araştırma & Akademi”, “Web Geliştirme/Frontend”).\n",
        "- Yetkinlik (skill): Varlığı metinde açıkça görülen ve pratikte uygulanabilir beceri/teknoloji.\n",
        "- Diller (spoken_languages): İnsan dilleri (örn: Türkçe, İngilizce, Almanca).\n",
        "\n",
        "### NORMALİZASYON\n",
        "- Eşanlamlıları birleştir: “HTML,CSS” → “HTML”, “CSS”; “JS” → “JavaScript”.\n",
        "- Adları büyük-küçük harf duyarsız tekilleştir: “python” → “Python”.\n",
        "- Alan adlarını üst kavramlara eşle (örn: “LLM”, “VLM” → “Yapay Zeka / Büyük Dil Modelleri”).\n",
        "\n",
        "### DEĞERLEME/PUANLAMA\n",
        "Her skill için:\n",
        "- level ∈ {“basic”, “intermediate”, “advanced”}\n",
        "- confidence ∈ [0.0, 1.0]\n",
        "Seviye sezgileri:\n",
        "- CV’nin “Yetkinlikler” listesinde geçiyorsa: en az “basic”.\n",
        "- Proje/tecrübe bölümünde somut kullanımla geçiyorsa: “intermediate”.\n",
        "- Araştırma odaklı derin kullanım, ince ayar (fine-tuning), yarışma/üretim çıktısı vb. varsa: “advanced”.\n",
        "Confidence, kanıt yoğunluğuna göre artar.\n",
        "\n",
        "### KANIT\n",
        "Her interest ve skill için metinden en az 1 “evidence” ver:\n",
        "- `quote`: metinden kısa alıntı\n",
        "- `section`: bulunduğu bölüm adı (örn: “Yetkinlikler”, “Tecrübeler”, “Projeler”)\n",
        "- (varsa) `lines` ya da yaklaşık karakter aralığı.\n",
        "\n",
        "### KATI JSON ŞEMASI\n",
        "{\n",
        "  \"skills\": [\n",
        "    {\n",
        "      \"name\": string,\n",
        "      \"type\": \"programming_language\" | \"framework_library\" | \"tool\" | \"domain_knowledge\" | \"soft_skill\" | \"human_language\",\n",
        "      \"level\": number, // basic=1, intermediate=3, advanced=5 1-5\n",
        "      \"confidence\": number,\n",
        "      \"evidence\": [{ \"quote\": string, \"section\": string }]\n",
        "    }\n",
        "  ],\n",
        "  \"spoken_languages\": [\n",
        "    {\n",
        "      \"language\": string, // Türkçe=tr, İngilizce=en, Almanca=de, Fransızca=fr\n",
        "      \"confidence\": number,\n",
        "      \"evidence\": [{ \"quote\": string, \"section\": string }]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "### KURALLAR\n",
        "- SADECE JSON döndür.\n",
        "- Metinde olmayanı UYDURMA. Şüpheli isen confidence’ı düşür.\n",
        "- En fazla 10 interest ve 30 skill döndür.\n",
        "- “human_language” türündeki yetkinlikleri `spoken_languages` alanında da listele.\n",
        "\n",
        "### GİRDİ\n",
        "{{cv_text}}\n",
        "\n",
        "### ÇIKTI\n",
        "SADECE geçerli JSON.\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------\n",
        "# 2) CV metni (pipeline'dan)\n",
        "# -----------------------------\n",
        "try:\n",
        "    cv_text = full_text  # mevcut pipeline değişkeniniz\n",
        "except Exception:\n",
        "    cv_text = None\n",
        "\n",
        "if not cv_text or not cv_text.strip():\n",
        "    print(\"❌ 'cv_text' boş veya erişilemedi. Lütfen cv_text'i doldurun.\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "# **ÖNCE CV tespiti yap**\n",
        "ensure_cv_or_exit(cv_text, threshold=0.45)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) API isteği\n",
        "# -----------------------------\n",
        "url = \"https://leopard-loyal-monkfish.ngrok-free.app/v1/chat/completions\"\n",
        "\n",
        "payload = {\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"CV Metni:\\n\\n{cv_text}\\n\\nYukarıdaki CV'den ilgi alanları ve yetkinlikleri JSON formatında çıkar.\"\n",
        "        },\n",
        "    ],\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 4096\n",
        "}\n",
        "\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "print(\"🔍 CV analiz ediliyor...\")\n",
        "try:\n",
        "    response = requests.post(url, headers=headers, json=payload, timeout=120)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        content = result[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "        print(\"\\n📄 Model Yanıtı:\")\n",
        "        print(\"=\" * 80)\n",
        "        print(content)\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Önce doğrudan parse etmeyi dene\n",
        "        parsed_data = None\n",
        "        try:\n",
        "            parsed_data = json.loads(content)\n",
        "        except json.JSONDecodeError:\n",
        "            # Fallback: dıştaki ilk {} bloğunu çek\n",
        "            m = re.search(r\"\\{[\\s\\S]*\\}\", content)\n",
        "            if m:\n",
        "                parsed_data = json.loads(m.group())\n",
        "\n",
        "        if parsed_data is not None:\n",
        "            print(\"\\n✅ Parse Edilmiş JSON:\")\n",
        "            print(json.dumps(parsed_data, indent=2, ensure_ascii=False))\n",
        "\n",
        "            with open(\"cv_analysis.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(parsed_data, f, indent=2, ensure_ascii=False)\n",
        "            print(\"\\n📁 Sonuçlar 'cv_analysis.json' dosyasına kaydedildi.\")\n",
        "        else:\n",
        "            print(\"⚠️ JSON bulunamadı veya parse edilemedi.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"❌ HTTP Hatası: {response.status_code}\")\n",
        "        print(response.text)\n",
        "\n",
        "except requests.exceptions.Timeout:\n",
        "    print(\"❌ İstek zaman aşımına uğradı (120 saniye)\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Hata: {e}\")"
      ],
      "metadata": {
        "id": "wEAhOydHfF8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Ip_Xyfr6eKt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}